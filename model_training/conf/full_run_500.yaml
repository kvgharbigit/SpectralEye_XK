# Complete configuration for 500x500 full training run

defaults:
  - /config
  - override /model: mae_tiny
  - override /dataset: combined_dataset
  - override /scheduler: cosine_annealing

# Optimized hyperparameters for 500x500 full training (from diagnostic results)
hparams:
  batch_size: 6  # Optimal per-GPU batch size (w2_b6 was fastest: 48.1 samples/sec)
  nb_epochs: 250
  lr: 0.0001
  valid_interval: 10  # Validation every 10 epochs for long training

# Full dataset training
dataset:
  trial_mode: false

# Enable DDP for faster training
general:
  use_ddp: true
  parallel:
    use_parallel: true
    device_ids: [0, 1, 2]

# Standard data loading for 500x500
dataloader:
  num_workers: 2  # Moderate for larger files
  pin_memory: true
  prefetch_factor: 2